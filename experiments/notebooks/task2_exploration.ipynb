{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"da4e4ee469304c42a5ebdef814c2ec9f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_05dff553110945b5b871bd7f090d93f5","IPY_MODEL_afefd0fbdb174530b0b8e28f25aa7969","IPY_MODEL_18f83c9048204bd982cfb2e1047c757c"],"layout":"IPY_MODEL_7669ed5c7f544d929b0ad2fa3d0d1a4a"}},"05dff553110945b5b871bd7f090d93f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32ef2d9f8e364108b6670edc407dd6ec","placeholder":"​","style":"IPY_MODEL_1ed35e3070a94e729b505f29fb9d3f46","value":"Map: 100%"}},"afefd0fbdb174530b0b8e28f25aa7969":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60c091a26b9e41f8ba767e1e2115f13b","max":3103,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21609825b32f4025831ff601f67676ec","value":3103}},"18f83c9048204bd982cfb2e1047c757c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5e5c3c6b02a4d8f96dc7fa61e3db164","placeholder":"​","style":"IPY_MODEL_867f2d327adc4958b553b0960f2cf071","value":" 3103/3103 [00:02&lt;00:00, 1272.97 examples/s]"}},"7669ed5c7f544d929b0ad2fa3d0d1a4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32ef2d9f8e364108b6670edc407dd6ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ed35e3070a94e729b505f29fb9d3f46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60c091a26b9e41f8ba767e1e2115f13b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21609825b32f4025831ff601f67676ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5e5c3c6b02a4d8f96dc7fa61e3db164":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"867f2d327adc4958b553b0960f2cf071":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebb18985c6eb4b9c89d3966be2946c3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c371fe6c298e4c5597e79e3c1ff72c0f","IPY_MODEL_3f6435ceb2174e3f921efb7d873849f8","IPY_MODEL_0ba4f1de12534c139270c1c74338300f"],"layout":"IPY_MODEL_ddf226fcbedd4fcd8dc45b573ca4218f"}},"c371fe6c298e4c5597e79e3c1ff72c0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3e25c2961b3429d80f907c0c9f96d4f","placeholder":"​","style":"IPY_MODEL_05318a6989f444e2a42b4d640ae005f0","value":"Map: 100%"}},"3f6435ceb2174e3f921efb7d873849f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6afc77d62e834f04ac231a1923013897","max":345,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cdefd22ebc2e41d8866d020103dee97b","value":345}},"0ba4f1de12534c139270c1c74338300f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9c4de13bd674ff8b7731a3bb90e75c5","placeholder":"​","style":"IPY_MODEL_a807072866d4442d89365aedcf63fc39","value":" 345/345 [00:00&lt;00:00, 741.32 examples/s]"}},"ddf226fcbedd4fcd8dc45b573ca4218f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3e25c2961b3429d80f907c0c9f96d4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05318a6989f444e2a42b4d640ae005f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6afc77d62e834f04ac231a1923013897":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdefd22ebc2e41d8866d020103dee97b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9c4de13bd674ff8b7731a3bb90e75c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a807072866d4442d89365aedcf63fc39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6f07ae8d3d542f198d7a0e11f0e7f9c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df5b7a40ff3d4d399e39bb11f57ac963","IPY_MODEL_aa9b90ff893347d68b697191b0c77505","IPY_MODEL_4aae75264ebe471e9e785b61fba138f9"],"layout":"IPY_MODEL_1d9c025364df4a7eb10c9d1c3fb32d2d"}},"df5b7a40ff3d4d399e39bb11f57ac963":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea455a43b6a54512a6097edbc7c0f15b","placeholder":"​","style":"IPY_MODEL_8256d93b4377424aabe2c4498263c45b","value":"Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"}},"aa9b90ff893347d68b697191b0c77505":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e4cb92f053548d19a10e0ffe5c7c193","max":345,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6148bbba55dd496faa648a85c280c158","value":345}},"4aae75264ebe471e9e785b61fba138f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_330dafacd33142db8c4424a64f0b5bf7","placeholder":"​","style":"IPY_MODEL_a4b5de98dd404122b557fe24633d6ada","value":" 345/345 [00:03&lt;00:00, 60.90 examples/s]"}},"1d9c025364df4a7eb10c9d1c3fb32d2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea455a43b6a54512a6097edbc7c0f15b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8256d93b4377424aabe2c4498263c45b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e4cb92f053548d19a10e0ffe5c7c193":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6148bbba55dd496faa648a85c280c158":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"330dafacd33142db8c4424a64f0b5bf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4b5de98dd404122b557fe24633d6ada":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14823490,"sourceType":"datasetVersion","datasetId":9480062},{"sourceId":14851005,"sourceType":"datasetVersion","datasetId":9499051},{"sourceId":749853,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":572715,"modelId":585071},{"sourceId":749855,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":572717,"modelId":585072},{"sourceId":752407,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":572715,"modelId":585071}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxmLms-Nulvl","outputId":"0081676b-99cc-4d0a-f80e-e28b781510a9","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:14:19.713795Z","iopub.execute_input":"2026-02-15T21:14:19.714521Z","iopub.status.idle":"2026-02-15T21:17:49.602506Z","shell.execute_reply.started":"2026-02-15T21:14:19.714491Z","shell.execute_reply":"2026-02-15T21:17:49.601649Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2026.2.1-py3-none-any.whl.metadata (69 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2026.2.1 (from unsloth)\n  Downloading unsloth_zoo-2026.2.1-py3-none-any.whl.metadata (32 kB)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (26.0rc2)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\nCollecting tyro (from unsloth)\n  Downloading tyro-1.0.6-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.34-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\nCollecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth)\n  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\nCollecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth)\n  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\nCollecting peft!=0.11.0,>=0.18.0 (from unsloth)\n  Downloading peft-0.18.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\nRequirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\nCollecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth)\n  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.3)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\nCollecting multiprocess<0.70.17 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,!=4.57.4,!=4.57.5,<=4.57.6,>=4.51.3->unsloth) (0.22.1)\nCollecting torchao>=0.13.0 (from unsloth_zoo>=2026.2.1->unsloth)\n  Downloading torchao-0.16.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (20 kB)\nCollecting cut_cross_entropy (from unsloth_zoo>=2026.2.1->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.2.1->unsloth) (11.3.0)\nCollecting msgspec (from unsloth_zoo>=2026.2.1->unsloth)\n  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\nCollecting cuda-bindings==12.9.4 (from torch>=2.4.0->unsloth)\n  Downloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.4.5)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cufft-cu12==11.3.0.4->torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nCollecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch>=2.4.0->unsloth)\n  Downloading cuda_pathfinder-1.3.4-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\nDownloading unsloth-2026.2.1-py3-none-any.whl (432 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.3/432.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.18.1-py3-none-any.whl (556 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2026.2.1-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.5/376.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.34-cp39-abi3-manylinux_2_28_x86_64.whl (110.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchvision-0.25.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-1.0.6-py3-none-any.whl (181 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchao-0.16.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cuda_pathfinder-1.3.4-py3-none-any.whl (30 kB)\nInstalling collected packages: torchao, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, fsspec, cuda-pathfinder, tyro, nvidia-cusparse-cu12, nvidia-cufft-cu12, cuda-bindings, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, peft, unsloth_zoo, unsloth\n  Attempting uninstall: torchao\n    Found existing installation: torchao 0.10.0\n    Uninstalling torchao-0.10.0:\n      Successfully uninstalled torchao-0.10.0\n  Attempting uninstall: triton\n    Found existing installation: triton 3.4.0\n    Uninstalling triton-3.4.0:\n      Successfully uninstalled triton-3.4.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.6.77\n    Uninstalling nvidia-nvtx-cu12-12.6.77:\n      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.27.3\n    Uninstalling nvidia-nccl-cu12-2.27.3:\n      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufile-cu12\n    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.18\n    Uninstalling multiprocess-0.70.18:\n      Successfully uninstalled multiprocess-0.70.18\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.10.0\n    Uninstalling fsspec-2025.10.0:\n      Successfully uninstalled fsspec-2025.10.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.8.0+cu126\n    Uninstalling torch-2.8.0+cu126:\n      Successfully uninstalled torch-2.8.0+cu126\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.2\n    Uninstalling datasets-4.4.2:\n      Successfully uninstalled datasets-4.4.2\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.23.0+cu126\n    Uninstalling torchvision-0.23.0+cu126:\n      Successfully uninstalled torchvision-0.23.0+cu126\n  Attempting uninstall: peft\n    Found existing installation: peft 0.17.1\n    Uninstalling peft-0.17.1:\n      Successfully uninstalled peft-0.17.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.10.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.49.1 cuda-bindings-12.9.4 cuda-pathfinder-1.3.4 cut_cross_entropy-25.1.1 datasets-4.3.0 fsspec-2025.9.0 msgspec-0.20.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 peft-0.18.1 torch-2.10.0 torchao-0.16.0 torchvision-0.25.0 triton-3.6.0 trl-0.24.0 tyro-1.0.6 unsloth-2026.2.1 unsloth_zoo-2026.2.1 xformers-0.0.34\nFound existing installation: unsloth 2026.2.1\nUninstalling unsloth-2026.2.1:\n  Successfully uninstalled unsloth-2026.2.1\nCollecting git+https://github.com/unslothai/unsloth.git\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-zz319teb\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-zz319teb\n  Resolved https://github.com/unslothai/unsloth.git to commit 56c8f9662b1bc1fb50bcbe6bcc45ddffb0cdeb60\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for unsloth: filename=unsloth-2026.2.1-py3-none-any.whl size=441477 sha256=660a6dd856c7c9bb00bce4fc70f81b290d946ff1137bdb9d5ea69407e27f6984\n  Stored in directory: /tmp/pip-ephem-wheel-cache-9jj41upz/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\nSuccessfully built unsloth\nInstalling collected packages: unsloth\nSuccessfully installed unsloth-2026.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# Force Python to only see the first GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:17:49.604336Z","iopub.execute_input":"2026-02-15T21:17:49.604553Z","iopub.status.idle":"2026-02-15T21:17:49.608382Z","shell.execute_reply.started":"2026-02-15T21:17:49.604530Z","shell.execute_reply":"2026-02-15T21:17:49.607833Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset, ClassLabel, Value\nimport yaml\n\n\ndef load_prompts(yaml_path=\"prompts.yaml\"):\n    with open(yaml_path, \"r\") as f:\n        return yaml.safe_load(f)\n\ndef transform_dataset_task(dataset, task, technique, tokenizer, yaml_path=\"/configs/prompts.yaml\"):\n    \"\"\"\n    Transforms the dataset by applying the specific prompt technique from the YAML.\n    \n    Args:\n        dataset (DatasetDict or Dataset): The Hugging Face dataset.\n        task (str): 'task_1_clarity' or 'task_2_evasion'.\n        technique (str): 'default', 'persona', 'chain_of_thought', 'definition_aware'.\n        yaml_path (str): Path to prompts.yaml.\n        \n    Returns:\n        Dataset: The dataset with a new 'text' column containing the formatted prompt.\n    \"\"\"\n    prompts = load_prompts(yaml_path)\n\n    # Validation\n    if task not in prompts:\n        raise ValueError(f\"Task '{task}' not found in yaml. Available: {list(prompts.keys())}\")\n    if technique not in prompts[task]:\n        raise ValueError(f\"Technique '{technique}' not found for {task}. Available: {list(prompts[task].keys())}\")\n        \n    config = prompts[task][technique]\n    system_prompt = config['system']\n    \n    if task == 'task_1_clarity':\n        label_column = \"clarity_label\"\n        unique_labels = sorted(set(dataset[\"train\"][label_column]))\n    elif task == 'task_2_evasion':\n        label_column = \"evasion_label\"\n        unique_labels = sorted(set(dataset[\"train\"][label_column]))\n    # Step 1: Temporarily cast to ClassLabel for stratification\n    \n    dataset = dataset['train'].cast_column(\n        label_column,\n        ClassLabel(names=unique_labels)\n    )\n    # Step 2: Stratified split\n    dataset = dataset.train_test_split(\n        test_size=0.1,\n        stratify_by_column=label_column,\n        seed=42,\n    )\n    dataset = dataset['test']\n\n    # Step 3: Convert labels BACK to strings\n    def convert_label_back(example):\n        idx = int(example[label_column])   # convert \"4\" → 4\n        example[label_column] = unique_labels[idx]\n        return example\n\n    dataset = dataset.cast_column(label_column, Value(\"string\"))\n    dataset = dataset.map(convert_label_back)\n        \n    def format_row(row):\n        # 1. Extract inputs\n        q = row['interview_question']\n        a = row['interview_answer']\n        p = row['president']\n        d = row['date']\n        raw_label = row[label_column]\n        \n    \n        # 2. Format the full string\n        # Note: We strip whitespace to prevent tokenization issues at boundaries\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": f\"President: {p}\\nDate: {d}\\n{q}\\n{a}\"}, #Q, A are already in the dataset before each Question and Answer respectively\n            {\"role\": \"assistant\", \"content\": str(raw_label)}\n            ]\n        \n        # 3. Apply Template\n        # tokenize=False gives you the formatted string back so you can debug it\n        # add_generation_prompt=False because we include the assistant response (training)\n        formatted_text = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=False\n        )\n        \n        return {\"text\": formatted_text}\n\n    # Apply transformation\n    # remove_columns is optional but recommended if you only need the 'text' for training\n    return dataset.map(format_row, remove_columns=dataset.column_names)","metadata":{"id":"S_kUP18cfOua","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:21:57.640134Z","iopub.execute_input":"2026-02-15T21:21:57.640670Z","iopub.status.idle":"2026-02-15T21:21:57.649820Z","shell.execute_reply.started":"2026-02-15T21:21:57.640627Z","shell.execute_reply":"2026-02-15T21:21:57.649118Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\ndef get_model_and_tokenizer(model_name=\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", max_seq_length=2048,\n                            dtype=None, load_in_4bit = True, output_dir='.'):\n    \"\"\"\n    Loads the 4-bit Quantized Mistral model with Unsloth optimizations.\n    Returns: (model, tokenizer) tuple\n    \"\"\"\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = model_name,\n        max_seq_length = max_seq_length,\n        load_in_4bit = load_in_4bit,    # Force 4-bit loading\n        dtype=dtype # Explicitly pass torch_dtype to address KeyError\n    )\n\n    # Attach LoRA Adapters (Configuration)\n    model = FastLanguageModel.get_peft_model(\n        model,\n        r = 16,\n        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n        lora_alpha = 16,\n        lora_dropout = 0,       # Optimized for Unsloth\n        bias = \"none\",\n        use_gradient_checkpointing = \"unsloth\",\n        random_state = 3407,\n    )\n    model.save_pretrained(output_dir)\n    return model, tokenizer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25boxscRhyum","outputId":"28dccb73-bd70-43ba-e191-e3bcac9fbfa6","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:17:54.233410Z","iopub.execute_input":"2026-02-15T21:17:54.233843Z","iopub.status.idle":"2026-02-15T21:18:38.113761Z","shell.execute_reply.started":"2026-02-15T21:17:54.233819Z","shell.execute_reply":"2026-02-15T21:18:38.113151Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2026-02-15 21:18:02.443274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771190282.835007      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771190282.957932      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771190283.905004      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771190283.905038      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771190283.905041      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771190283.905043      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\nmodel, tokenizer = get_model_and_tokenizer()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a270lgZthaY","outputId":"7ef3ddfb-b8e5-4233-8089-81d7aa5d780f","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:20:51.200880Z","iopub.execute_input":"2026-02-15T21:20:51.201674Z","iopub.status.idle":"2026-02-15T21:21:18.097510Z","shell.execute_reply.started":"2026-02-15T21:20:51.201629Z","shell.execute_reply":"2026-02-15T21:21:18.096804Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2026.2.1: Fast Mistral patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7edc95f97a0840ec9af75f6f1292a0a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/157 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d303cd89b3242558a4a6fabc8639246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e32ff9563984684b73c30030ed310bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b924371214404da2998ba36f84d3351c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b877ba495294910911a2b036be6e44c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa909514ccba4533be21af5b7e071ffe"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2026.2.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"tokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"mistral\", # Forces Mistral v0.3 format\n    )","metadata":{"id":"u_w70QcNjC9G","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:21:18.099141Z","iopub.execute_input":"2026-02-15T21:21:18.099511Z","iopub.status.idle":"2026-02-15T21:21:18.104127Z","shell.execute_reply.started":"2026-02-15T21:21:18.099470Z","shell.execute_reply":"2026-02-15T21:21:18.103312Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dataset = load_dataset(\"ailsntua/QEvasion\")\nsubset = transform_dataset_task(dataset, 'task_2_evasion', 'chain_of_thought', tokenizer, \"/kaggle/input/datasets/ostriftis/prompts1/prompts.yaml\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"1aNr3aqEmKcM","outputId":"17e6d873-3482-47d9-efbf-6af5159affc6","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T18:58:01.177318Z","iopub.execute_input":"2026-02-15T18:58:01.178066Z","iopub.status.idle":"2026-02-15T18:58:01.818673Z","shell.execute_reply.started":"2026-02-15T18:58:01.178025Z","shell.execute_reply":"2026-02-15T18:58:01.817771Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98f55e19f36248fa98f5b280ea03c54b"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"print(subset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T18:58:05.117871Z","iopub.execute_input":"2026-02-15T18:58:05.118674Z","iopub.status.idle":"2026-02-15T18:58:05.123344Z","shell.execute_reply.started":"2026-02-15T18:58:05.118626Z","shell.execute_reply":"2026-02-15T18:58:05.122502Z"}},"outputs":[{"name":"stdout","text":"{'text': \"<s>[INST] First, explain whether the politician answers directly or avoids the question. Think step by step, but keep your reasoning separate from the final answer.\\nThen, classify the evasion level of the answer as 'Explicit', 'Implicit', 'Dodging', 'General', 'Deflection', 'Partial/half-answer', 'Declining to answer', 'Claims ignorance', and 'Clarification'.\\n President: Barack Obama\\nDate: August 01, 2014\\nQ. Mr. President, like that cease-fire, you've called for diplomatic solutions not only in Israel and Gaza, but also in Ukraine, in Iraq, to very little effect so far. Has the United States of America lost its influence in the world? Have you lost yours?\\nYes. Look, this is a common theme that folks bring up. Apparently, people have forgotten that America, as the most powerful country on Earth, still does not control everything around the world. And so our diplomatic efforts often take time. They often will see progress and then a step backwards. That's been true in the Middle East. That's been true in Europe. That's been true in Asia. That's the nature of world affairs. It's not neat, and it's not smooth.But if you look at, for example, Ukraine, we have made progress in delivering on what we said we would do. We can't control how Mr. Putin thinks. But what we can do is say to Mr. Putin, if you continue on the path of arming separatists with heavy armaments that, the evidence suggests, may have resulted in 300 innocent people on a jet dying, and that violates international law and undermines the integrity—territorial integrity and sovereignty of Ukraine, then you're going to face consequences that will hurt your country.And there was a lot of skepticism about our ability to coordinate with Europeans for a strong series of sanctions. And each time, we have done what we said we would do, including this week, when we put in place sanctions that have an impact on key sectors of the Russian economy: their energy, their defense, and their financial systems.It hasn't resolved the problem yet. I spoke to Mr. Putin this morning, and I indicated to him, just as we will do what we say we do in terms of sanctions, we'll also do what we say we do in terms of wanting to resolve this issue diplomatically if he takes a different position. If he respects and honors the right of Ukrainians to determine their own destiny, then it's possible to make sure that Russian interests are addressed that are legitimate and that Ukrainians are able to make their own decisions and we can resolve this conflict and end some of the bloodshed.But the point is, though, Bill, that if you look at the 20th century and the early part of this century, there are a lot of conflicts that America doesn't resolve. That's always been true. That doesn't mean we stop trying. And it's not a measure of American influence on any given day or at any given moment that there are conflicts around the world that are difficult. The conflict in Northern Ireland raged for a very, very long time until finally something broke, where the parties decided that it wasn't worth killing each other.The Palestinian-Israeli conflicts has been going on even longer than you've been reporting. [] And I don't think at any point was there a suggestion somehow that America didn't have influence just because we weren't able to finalize an Israeli-Palestinian peace deal. You will recall that situations like Kosovo and Bosnia raged on for quite some time, and there was a lot more death and bloodshed than there has been so far in the Ukrainian situation before it ultimately did get resolved.And so I recognize, with so many different issues popping up around the world, sometimes, it may seem as if this is an aberration or it's unusual. But the truth of the matter is, is that this is a big world out there and that as indispensable as we are to try to lead it, there's still going to be tragedies out there and there are going to be conflicts. And our job is to just make sure that we continue to project what's right, what's just, and that we're building coalitions of like-minded countries and partners in order to advance not only our core security interests, but also the interests of the world as a whole. [/INST]Dodging</s>\"}\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"del dataset","metadata":{"id":"gthTGUlr4KEt","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T03:45:00.063515Z","iopub.execute_input":"2026-02-13T03:45:00.063813Z","iopub.status.idle":"2026-02-13T03:45:00.068150Z","shell.execute_reply.started":"2026-02-13T03:45:00.063738Z","shell.execute_reply":"2026-02-13T03:45:00.067339Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\n\ndef train_model(model, tokenizer, dataset, output_dir='lora.pth', \n                training_args = TrainingArguments(\n                    per_device_train_batch_size = 1,\n                    gradient_accumulation_steps = 8,\n                    warmup_steps = 5,\n                    num_train_epochs = 1, # Set this for 1 full training run.\n                    learning_rate = 2e-4,\n                    fp16 = not is_bfloat16_supported(),\n                    bf16 = is_bfloat16_supported(),\n                    logging_steps = 1,\n                    optim = \"adamw_8bit\",\n                    weight_decay = 0.01,\n                    lr_scheduler_type = \"linear\",\n                    seed = 21,\n                    output_dir = \"outputs\",\n                    report_to = \"none\", # Use this for WandB etc\n                    )\n                ):\n    \"\"\"\n    Executes the training loop.\n    Args:\n        model: Pre-loaded Unsloth model object\n        tokenizer: Pre-loaded Unsloth tokenizer\n        output_dir: Where to save the adapter\n        subset_size: Limit for pilot testing\n    \"\"\"\n        \n    trainer = SFTTrainer(\n        model = model,\n        tokenizer = tokenizer,\n        train_dataset = dataset,\n        dataset_text_field = \"text\",\n        max_seq_length = 2048,\n        dataset_num_proc = 2,\n        packing = False,\n        args = training_args,\n    )\n    torch.cuda.empty_cache()\n\n    trainer.train()\n    print(f\"Saving adapter to {output_dir}\")\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)","metadata":{"id":"RmW_v5WSlAUL","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:21:18.104999Z","iopub.execute_input":"2026-02-15T21:21:18.105294Z","iopub.status.idle":"2026-02-15T21:21:18.160840Z","shell.execute_reply.started":"2026-02-15T21:21:18.105258Z","shell.execute_reply":"2026-02-15T21:21:18.160340Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_model(model, tokenizer, dataset=subset)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669,"referenced_widgets":["a6f07ae8d3d542f198d7a0e11f0e7f9c","df5b7a40ff3d4d399e39bb11f57ac963","aa9b90ff893347d68b697191b0c77505","4aae75264ebe471e9e785b61fba138f9","1d9c025364df4a7eb10c9d1c3fb32d2d","ea455a43b6a54512a6097edbc7c0f15b","8256d93b4377424aabe2c4498263c45b","6e4cb92f053548d19a10e0ffe5c7c193","6148bbba55dd496faa648a85c280c158","330dafacd33142db8c4424a64f0b5bf7","a4b5de98dd404122b557fe24633d6ada"]},"id":"2vjTEhlBzVpz","outputId":"8ede4db7-cfb6-482c-f736-f9d6dd445c75","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T18:59:36.652708Z","iopub.execute_input":"2026-02-15T18:59:36.653544Z","iopub.status.idle":"2026-02-15T19:10:13.588251Z","shell.execute_reply.started":"2026-02-15T18:59:36.653469Z","shell.execute_reply":"2026-02-15T19:10:13.587465Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"359ee4b427104539bd6645f4f0ecf684"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 345 | Num Epochs = 1 | Total steps = 44\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040 of 7,289,966,592 (0.58% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [44/44 10:04, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.525900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.617900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.459800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.321800</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.058100</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.091600</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.911400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.785600</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.714600</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.715900</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.856800</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.681900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.815600</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.859500</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.567100</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.727900</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.615600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.666200</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.775200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.553200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.739400</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.588000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.632200</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.631500</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.647900</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.574600</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.533400</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.494900</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.754400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.726900</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.537900</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.717100</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.578500</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.634600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.686600</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.598700</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.504100</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.547600</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.688700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.680500</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.394800</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.518200</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.538400</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.219000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving adapter to lora.pth\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# 1. Compress the lora.pth folder into a single zip file\nshutil.make_archive('lora_weights', 'zip', '/kaggle/working/lora.pth')\n\n# 2. Generate a clickable download link for the zipped file\nFileLink(r'lora_weights.zip')","metadata":{"id":"gnp40C9h1_BJ","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:10:27.387309Z","iopub.execute_input":"2026-02-15T19:10:27.388075Z","iopub.status.idle":"2026-02-15T19:10:36.089329Z","shell.execute_reply.started":"2026-02-15T19:10:27.388029Z","shell.execute_reply":"2026-02-15T19:10:36.088620Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/lora_weights.zip","text/html":"<a href='lora_weights.zip' target='_blank'>lora_weights.zip</a><br>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"del model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:27:49.322125Z","iopub.execute_input":"2026-02-15T19:27:49.322902Z","iopub.status.idle":"2026-02-15T19:27:49.326697Z","shell.execute_reply.started":"2026-02-15T19:27:49.322849Z","shell.execute_reply":"2026-02-15T19:27:49.325850Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:27:50.499054Z","iopub.execute_input":"2026-02-15T19:27:50.499896Z","iopub.status.idle":"2026-02-15T19:27:50.572779Z","shell.execute_reply.started":"2026-02-15T19:27:50.499850Z","shell.execute_reply":"2026-02-15T19:27:50.571912Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"model, tokenizer = get_model_and_tokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:27:52.372564Z","iopub.execute_input":"2026-02-15T19:27:52.373329Z","iopub.status.idle":"2026-02-15T19:28:09.784117Z","shell.execute_reply.started":"2026-02-15T19:27:52.373287Z","shell.execute_reply":"2026-02-15T19:28:09.783063Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2026.2.1: Fast Mistral patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"dataset = load_dataset(\"ailsntua/QEvasion\")\nsubset = transform_dataset_task(dataset, 'task_2_evasion', 'default', tokenizer, \"/kaggle/input/datasets/ostriftis/prompts1/prompts.yaml\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:28:16.403204Z","iopub.execute_input":"2026-02-15T19:28:16.404036Z","iopub.status.idle":"2026-02-15T19:28:17.178506Z","shell.execute_reply.started":"2026-02-15T19:28:16.403987Z","shell.execute_reply":"2026-02-15T19:28:17.177888Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8950ba5c9d44574aef78759e5fe8120"}},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"train_model(model, tokenizer, dataset=subset, output_dir='task2_default')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:29:01.849295Z","iopub.execute_input":"2026-02-15T19:29:01.849771Z","iopub.status.idle":"2026-02-15T19:38:13.656841Z","shell.execute_reply.started":"2026-02-15T19:29:01.849727Z","shell.execute_reply":"2026-02-15T19:38:13.655881Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19560952f23f41419a510c190934f588"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 345 | Num Epochs = 1 | Total steps = 44\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040 of 7,289,966,592 (0.58% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [44/44 08:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.430000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.520200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.394700</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.381000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.160700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.257400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.177100</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.089500</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.070100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.004500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.138800</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.053700</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.176300</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.234000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.952900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.008900</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.004000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.979900</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.014700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.895400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.950600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.883200</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.969000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.040600</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.953700</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.871100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.904400</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.017700</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.959700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.996700</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.792600</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.985500</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.927200</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.007800</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.957600</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.965200</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.773100</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.952900</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.925700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.010500</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.744200</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.795200</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.915500</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.546800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving adapter to task2_default\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# 1. Compress the lora.pth folder into a single zip file\nshutil.make_archive('lora_weights', 'zip', '/kaggle/working/task2_default')\n\n# 2. Generate a clickable download link for the zipped file\nFileLink(r'lora_weights.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:39:29.490624Z","iopub.execute_input":"2026-02-15T19:39:29.491004Z","iopub.status.idle":"2026-02-15T19:39:38.298756Z","shell.execute_reply.started":"2026-02-15T19:39:29.490963Z","shell.execute_reply":"2026-02-15T19:39:38.297937Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/lora_weights.zip","text/html":"<a href='lora_weights.zip' target='_blank'>lora_weights.zip</a><br>"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"del model, tokenizer\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:02:57.506209Z","iopub.execute_input":"2026-02-15T20:02:57.507142Z","iopub.status.idle":"2026-02-15T20:02:57.579236Z","shell.execute_reply.started":"2026-02-15T20:02:57.507066Z","shell.execute_reply":"2026-02-15T20:02:57.578314Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"model, tokenizer = get_model_and_tokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:03:00.376754Z","iopub.execute_input":"2026-02-15T20:03:00.377410Z","iopub.status.idle":"2026-02-15T20:03:18.113378Z","shell.execute_reply.started":"2026-02-15T20:03:00.377365Z","shell.execute_reply":"2026-02-15T20:03:18.112383Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2026.2.1: Fast Mistral patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"dataset = load_dataset(\"ailsntua/QEvasion\")\nsubset = transform_dataset_task(dataset, 'task_2_evasion', 'persona', tokenizer, \"/kaggle/input/datasets/ostriftis/prompts1/prompts.yaml\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:04:29.601722Z","iopub.execute_input":"2026-02-15T20:04:29.602756Z","iopub.status.idle":"2026-02-15T20:04:30.411957Z","shell.execute_reply.started":"2026-02-15T20:04:29.602685Z","shell.execute_reply":"2026-02-15T20:04:30.411053Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a43cb7589f734e668affa905231074c7"}},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"train_model(model, tokenizer, dataset=subset, output_dir='task2_persona')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:05:02.945448Z","iopub.execute_input":"2026-02-15T20:05:02.946337Z","iopub.status.idle":"2026-02-15T20:14:23.543405Z","shell.execute_reply.started":"2026-02-15T20:05:02.946288Z","shell.execute_reply":"2026-02-15T20:14:23.542402Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63051fdab27b433dae947505a5973d88"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 345 | Num Epochs = 1 | Total steps = 44\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040 of 7,289,966,592 (0.58% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [44/44 08:57, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.430000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.520200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.394800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.381000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.160700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.257500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.177100</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.089700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.070100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.004400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.138800</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.053800</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.176000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.234400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.952700</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.008600</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.003600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.980100</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.014700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.895600</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.950400</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.883300</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.969200</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.039800</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.953800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.871800</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.905400</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.018600</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.960400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.996700</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.792200</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.983700</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.928100</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.007300</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.958700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.965200</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.772900</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.952500</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.927200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.011400</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.745400</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.794400</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.915100</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>1.544400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving adapter to task2_persona\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"shutil.make_archive('task2_persona', 'zip', '/kaggle/working/task2_persona')\n\n# 2. Generate a clickable download link for the zipped file\nFileLink(r'task2_persona.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:14:41.186020Z","iopub.execute_input":"2026-02-15T20:14:41.186410Z","iopub.status.idle":"2026-02-15T20:14:49.831659Z","shell.execute_reply.started":"2026-02-15T20:14:41.186365Z","shell.execute_reply":"2026-02-15T20:14:49.830798Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/task2_persona.zip","text/html":"<a href='task2_persona.zip' target='_blank'>task2_persona.zip</a><br>"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"del model, tokenizer\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T04:35:53.660696Z","iopub.execute_input":"2026-02-13T04:35:53.661836Z","iopub.status.idle":"2026-02-13T04:35:53.665923Z","shell.execute_reply.started":"2026-02-13T04:35:53.661766Z","shell.execute_reply":"2026-02-13T04:35:53.665144Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model, tokenizer = get_model_and_tokenizer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:18:38.123985Z","iopub.status.idle":"2026-02-15T21:18:38.124466Z","shell.execute_reply.started":"2026-02-15T21:18:38.124201Z","shell.execute_reply":"2026-02-15T21:18:38.124255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(\"ailsntua/QEvasion\")\nsubset = transform_dataset_task(dataset, 'task_2_evasion', \n                                'definition_aware', tokenizer, \"/kaggle/input/datasets/ostriftis/prompts1/prompts.yaml\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:22:08.230722Z","iopub.execute_input":"2026-02-15T21:22:08.231103Z","iopub.status.idle":"2026-02-15T21:22:09.079015Z","shell.execute_reply.started":"2026-02-15T21:22:08.231062Z","shell.execute_reply":"2026-02-15T21:22:09.078215Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15086d02b2f24208bc6a609d1bc4a3b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a87b5eb3598d42d7be93d77ba2d41e97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f3ff6953bc6463fa036b785d5a72d03"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"train_model(model, tokenizer, dataset=subset, output_dir= \"task2_definition\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:22:20.463459Z","iopub.execute_input":"2026-02-15T21:22:20.464068Z","iopub.status.idle":"2026-02-15T21:32:36.667860Z","shell.execute_reply.started":"2026-02-15T21:22:20.464021Z","shell.execute_reply":"2026-02-15T21:32:36.666969Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/345 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab60121b3be4255b4812f91ba6ff8af"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 345 | Num Epochs = 1 | Total steps = 44\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040 of 7,289,966,592 (0.58% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [44/44 09:50, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.430500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.510900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.298200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.117200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.859800</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.820500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.591200</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.463000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.363200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.413800</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.533400</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.314700</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.426600</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.486600</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.221000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.425600</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.282000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.351300</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.519400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.239400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.480500</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.289300</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.315400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.286000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.342500</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.280500</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.192300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.115100</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>1.464700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.429400</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>1.267700</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.428000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>1.234800</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>1.278100</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.372900</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>1.258200</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>1.232600</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>1.186600</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>1.409200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.349300</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>1.099400</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>1.213300</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>1.192500</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.939700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving adapter to task2_definition\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\nshutil.make_archive('task2_definition', 'zip', '/kaggle/working/task2_definition')\n\n# 2. Generate a clickable download link for the zipped file\nFileLink(r'task2_definition.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:33:28.690877Z","iopub.execute_input":"2026-02-15T21:33:28.691256Z","iopub.status.idle":"2026-02-15T21:33:37.176115Z","shell.execute_reply.started":"2026-02-15T21:33:28.691216Z","shell.execute_reply":"2026-02-15T21:33:37.175437Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/task2_definition.zip","text/html":"<a href='task2_definition.zip' target='_blank'>task2_definition.zip</a><br>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"ailsntua/QEvasion\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T14:02:54.805127Z","iopub.execute_input":"2026-02-15T14:02:54.805832Z","iopub.status.idle":"2026-02-15T14:02:57.330785Z","shell.execute_reply.started":"2026-02-15T14:02:54.805786Z","shell.execute_reply":"2026-02-15T14:02:57.330076Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3cb1a8f3dec4839be75f651e01947f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/3.90M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63c9ed3b550d431a9cf0bd0df536458e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71148fc7b624e30ba30608756542a35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3448 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92da987b9d4c46c58045a079b319b3f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97bbfe0675b745bc9d44c8479776ae49"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_dataset = dataset['train']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T12:38:58.853852Z","iopub.execute_input":"2026-02-15T12:38:58.854139Z","iopub.status.idle":"2026-02-15T12:38:58.857735Z","shell.execute_reply.started":"2026-02-15T12:38:58.854114Z","shell.execute_reply":"2026-02-15T12:38:58.857123Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from tqdm import tqdm\nfrom collections import Counter\n\ndef predict_label(question, answer, president, date, task_type, technique, model, tokenizer):\n    \"\"\"Generates a classification label for a given question/answer pair.\"\"\"\n    \n    # Set the correct system prompt based on the task\n    prompts = load_prompts(\"/kaggle/input/datasets/ostriftis/prompts1/prompts.yaml\")\n    \n    sys_prompt = prompts[task_type][technique]\n    messages = [\n        {\"role\": \"system\", \"content\": sys_prompt['system']},\n        {\"role\": \"user\", \"content\": f\"President: {president}\\nDate: {date}\\n{question}\\n{answer}\"},\n    ]\n\n    # Tokenize and format with the chat template\n    formatted_prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    inputs = tokenizer(\n        formatted_prompt,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=2048,\n    ).to(\"cuda\")\n    # Generate the classification\n    outputs = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],  # CRITICAL: Pass the attention mask\n            max_new_tokens=50,  # Reduced since we expect short labels\n            do_sample=False,\n            temperature=1.0,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n\n    \n    # Slice the output tensor to isolate only the newly generated tokens\n    input_length = inputs[\"input_ids\"].shape[1]\n    new_tokens = outputs[0][input_length:]\n\n\n    # Slice the output tensor to isolate only the newly generated tokens\n    # This is much cleaner and less error-prone than string splitting!\n    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n\n\ndef evaluate_model_on_dataset(test_dataset, task_type, technique, model, tokenizer):\n    \"\"\"Iterates through the test dataset and collects predictions.\"\"\"\n    results = []\n    task_mapping = {'Clear Reply': ['Explicit'],\n                  'Ambivalent': ['Implicit', 'Dodging', 'General', 'Deflection', 'Partial/half-answer'],\n                  'Clear Non-Reply': ['Declining to answer', 'Claims ignorance', 'Clarification']}\n    \n    for row in tqdm(test_dataset):\n        question = row['interview_question']\n        answer = row['interview_answer']\n        clarity = row['clarity_label']\n        president = row['president']\n        date = row['date']\n        ground_truth = None\n        if task_type == 'task_1_clarity':\n            ground_truth = clarity\n        else:\n            valid_evasions = task_mapping.get(clarity, [])\n                \n            # Gather the 3 annotations\n            raw_votes = [row['annotator1'], row['annotator2'], row['annotator3']]\n            \n            # Filter step: only keep votes that exist in the valid mapping\n            valid_votes = [vote for vote in raw_votes if vote in valid_evasions]\n            \n            if valid_votes == []:\n                # Fallback for Zero Valid Votes: Pick the most common evasion tactic for that clarity class\n                fallbacks = {   \n                    'Clear Reply': 'Explicit',\n                    'Ambivalent': 'Dodging', \n                    'Clear Non-Reply': 'Declining to answer'\n                }\n                evasion = fallbacks.get(clarity, None)\n            else: \n                vote_counts = Counter(valid_votes)\n                most_common = vote_counts.most_common()\n                \n                evasion = most_common[0][0]\n                \n            ground_truth = evasion\n\n        prediction = predict_label(question, answer, president, date, task_type, technique, model, tokenizer)\n        \n        results.append({\n            \"question\": question,\n            \"answer\": answer,\n            \"ground_truth\": ground_truth,\n            \"prediction\": prediction\n        })\n        \n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:33:53.164298Z","iopub.execute_input":"2026-02-15T21:33:53.164645Z","iopub.status.idle":"2026-02-15T21:33:53.176106Z","shell.execute_reply.started":"2026-02-15T21:33:53.164605Z","shell.execute_reply":"2026-02-15T21:33:53.175436Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"del model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:33:59.963505Z","iopub.execute_input":"2026-02-15T21:33:59.964305Z","iopub.status.idle":"2026-02-15T21:33:59.967900Z","shell.execute_reply.started":"2026-02-15T21:33:59.964257Z","shell.execute_reply":"2026-02-15T21:33:59.967154Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    max_seq_length = 2048,\n    dtype = None,\n    load_in_4bit = True,\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# 2. Load your different trained adapters\n# model.load_adapter(\"/kaggle/working/lora.pth\", adapter_name=\"v2_cot\")\n# 3. Swap instantly to test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:34:00.710185Z","iopub.execute_input":"2026-02-15T21:34:00.710935Z","iopub.status.idle":"2026-02-15T21:34:10.407093Z","shell.execute_reply.started":"2026-02-15T21:34:00.710891Z","shell.execute_reply":"2026-02-15T21:34:10.406263Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2026.2.1: Fast Mistral patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32768, 4096, padding_idx=770)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n)"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"model.load_adapter(\"/kaggle/working/task2_default\", adapter_name=\"v2_default\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:40:57.629706Z","iopub.execute_input":"2026-02-15T19:40:57.630360Z","iopub.status.idle":"2026-02-15T19:40:58.434412Z","shell.execute_reply.started":"2026-02-15T19:40:57.630315Z","shell.execute_reply":"2026-02-15T19:40:58.433707Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"model.set_adapter(\"v2_default\")\nres['v2_default'] = evaluate_model_on_dataset(test_dataset=test_dataset, task_type='task_2_evasion',\n                                              technique='default', model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:42:16.186359Z","iopub.execute_input":"2026-02-15T19:42:16.187146Z","iopub.status.idle":"2026-02-15T19:47:52.472770Z","shell.execute_reply.started":"2026-02-15T19:42:16.187104Z","shell.execute_reply":"2026-02-15T19:47:52.471875Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 308/308 [05:36<00:00,  1.09s/it]\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"test_dataset = dataset['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:34:22.032516Z","iopub.execute_input":"2026-02-15T21:34:22.032889Z","iopub.status.idle":"2026-02-15T21:34:22.036923Z","shell.execute_reply.started":"2026-02-15T21:34:22.032839Z","shell.execute_reply":"2026-02-15T21:34:22.036056Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"res = {}\nmodel.set_adapter(\"v2_cot\")\nres['v2_cot'] = evaluate_model_on_dataset(test_dataset=test_dataset, task_type='task_2_evasion', \n                                          technique = 'chain_of_thought',model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:18:13.795131Z","iopub.execute_input":"2026-02-15T19:18:13.795470Z","iopub.status.idle":"2026-02-15T19:23:57.500743Z","shell.execute_reply.started":"2026-02-15T19:18:13.795429Z","shell.execute_reply":"2026-02-15T19:23:57.500038Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 308/308 [05:43<00:00,  1.12s/it]\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"model.load_adapter(\"/kaggle/working/task2_persona\", adapter_name=\"v2_persona\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:15:22.457397Z","iopub.execute_input":"2026-02-15T20:15:22.458312Z","iopub.status.idle":"2026-02-15T20:15:23.268284Z","shell.execute_reply.started":"2026-02-15T20:15:22.458262Z","shell.execute_reply":"2026-02-15T20:15:23.267337Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"model.set_adapter(\"v2_persona\")\nres['v2_persona'] = evaluate_model_on_dataset(test_dataset=test_dataset, task_type='task_2_evasion',\n                                              technique='persona', model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:15:23.269837Z","iopub.execute_input":"2026-02-15T20:15:23.270238Z","iopub.status.idle":"2026-02-15T20:21:08.328415Z","shell.execute_reply.started":"2026-02-15T20:15:23.270198Z","shell.execute_reply":"2026-02-15T20:21:08.327567Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 308/308 [05:45<00:00,  1.12s/it]\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"model.load_adapter(\"/kaggle/working/task2_definition\", adapter_name=\"v2_definition\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:34:10.408358Z","iopub.execute_input":"2026-02-15T21:34:10.408643Z","iopub.status.idle":"2026-02-15T21:34:11.220192Z","shell.execute_reply.started":"2026-02-15T21:34:10.408604Z","shell.execute_reply":"2026-02-15T21:34:11.219309Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"res = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:34:14.635677Z","iopub.execute_input":"2026-02-15T21:34:14.636269Z","iopub.status.idle":"2026-02-15T21:34:14.639699Z","shell.execute_reply.started":"2026-02-15T21:34:14.636221Z","shell.execute_reply":"2026-02-15T21:34:14.639102Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"model.set_adapter(\"v2_definition\")\nres['v2_definition'] = evaluate_model_on_dataset(test_dataset=test_dataset, task_type='task_2_evasion',\n                                              technique='definition_aware', model=model, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:34:27.480429Z","iopub.execute_input":"2026-02-15T21:34:27.480767Z","iopub.status.idle":"2026-02-15T21:40:11.279797Z","shell.execute_reply.started":"2026-02-15T21:34:27.480725Z","shell.execute_reply":"2026-02-15T21:40:11.278973Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 308/308 [05:43<00:00,  1.12s/it]\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report\n\ndef evaluate_f1_scores(results, valid_labels):\n    \"\"\"\n    Parses LLM predictions from a list of result dictionaries and calculates F1-scores.\n    \"\"\"\n    y_true = []\n    y_pred = []\n    \n    for row in results:\n        ground_truth = row['ground_truth']\n        raw_prediction = row['prediction']\n        \n        # 1. Safely Extract the Label\n        # We check if any of the valid labels are contained within the generated text.\n        # This prevents errors if the model outputs \"The label is: Deflection.\"\n        parsed_pred = \"Unknown\" # Fallback if the model hallucinated\n        for label in valid_labels:\n            if label.lower() in raw_prediction.lower():\n                parsed_pred = label\n                break\n                \n        y_true.append(ground_truth)\n        y_pred.append(parsed_pred)\n\n    # 2. Calculate F1 Scores\n    # zero_division=0 prevents warnings if a class was never predicted\n    macro_f1 = f1_score(y_true, y_pred, average='macro', labels=valid_labels, zero_division=0)\n    weighted_f1 = f1_score(y_true, y_pred, average='weighted', labels=valid_labels, zero_division=0)\n    \n    print(f\"Macro F1-Score:    {macro_f1:.4f}\")\n    print(f\"Weighted F1-Score: {weighted_f1:.4f}\\n\")\n    \n    # 3. Generate the Classification Report\n    # This builds a text report showing precision, recall, and F1 for each class.\n    print(\"--- Detailed Classification Report ---\")\n    print(classification_report(y_true, y_pred, labels=valid_labels, zero_division=0))\n    \n    return macro_f1, weighted_f1\n\n# --- How to use it with your data ---\n\n\n\n# Or for evasion:\n# print(\"Evaluating Evasion Task:\")\n# evaluate_f1_scores(evasion_results, evasion_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:40:11.281236Z","iopub.execute_input":"2026-02-15T21:40:11.281515Z","iopub.status.idle":"2026-02-15T21:40:11.288305Z","shell.execute_reply.started":"2026-02-15T21:40:11.281477Z","shell.execute_reply":"2026-02-15T21:40:11.287430Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"for i in range(10):\n    print(subset[i]['annotator1'], subset[i]['annotator3'], subset[i]['annotator2'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:17:32.094699Z","iopub.execute_input":"2026-02-15T19:17:32.095065Z","iopub.status.idle":"2026-02-15T19:17:32.108082Z","shell.execute_reply.started":"2026-02-15T19:17:32.095023Z","shell.execute_reply":"2026-02-15T19:17:32.107371Z"}},"outputs":[{"name":"stdout","text":"Dodging Dodging General\nDeflection General General\nExplicit Implicit Implicit\nExplicit General General\nDodging Dodging Implicit\nDodging Dodging Dodging\nDodging Dodging Dodging\nDodging Dodging Dodging\nGeneral General General\nImplicit Implicit Implicit\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"for i in range(10):\n    print(res['v2_cot'][i]['ground_truth'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:17:34.186078Z","iopub.execute_input":"2026-02-15T19:17:34.186436Z","iopub.status.idle":"2026-02-15T19:17:34.191697Z","shell.execute_reply.started":"2026-02-15T19:17:34.186392Z","shell.execute_reply":"2026-02-15T19:17:34.190604Z"}},"outputs":[{"name":"stdout","text":"Dodging\nGeneral\nImplicit\nGeneral\nDodging\nDodging\nDodging\nDodging\nGeneral\nImplicit\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Define the exact labels your model was trained to output\nclarity_labels = ['Clear Reply', 'Clear Non-Reply', 'Ambiguous']\nevasion_labels = ['Explicit', 'Implicit', 'Dodging', 'General', 'Deflection', \n                  'Partial/half-answer', 'Declining to answer', 'Claims ignorance', 'Clarification']\n\n# Assuming 'results' is the list returned from your inference loop:\nprint(\"Evaluating evasion Task:\")\nf1 = evaluate_f1_scores(res['v2_cot'], evasion_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T19:26:54.164256Z","iopub.execute_input":"2026-02-15T19:26:54.164767Z","iopub.status.idle":"2026-02-15T19:26:54.189601Z","shell.execute_reply.started":"2026-02-15T19:26:54.164723Z","shell.execute_reply":"2026-02-15T19:26:54.188911Z"}},"outputs":[{"name":"stdout","text":"Evaluating evasion Task:\nMacro F1-Score:    0.0647\nWeighted F1-Score: 0.1395\n\n--- Detailed Classification Report ---\n                     precision    recall  f1-score   support\n\n           Explicit       0.27      0.94      0.41        79\n           Implicit       0.22      0.10      0.14        61\n            Dodging       0.33      0.02      0.03        61\n            General       0.00      0.00      0.00        55\n         Deflection       0.00      0.00      0.00        22\nPartial/half-answer       0.00      0.00      0.00         7\nDeclining to answer       0.00      0.00      0.00        11\n   Claims ignorance       0.00      0.00      0.00         8\n      Clarification       0.00      0.00      0.00         4\n\n           accuracy                           0.26       308\n          macro avg       0.09      0.12      0.06       308\n       weighted avg       0.18      0.26      0.14       308\n\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# Assuming 'results' is the list returned from your inference loop:\nprint(\"Evaluating evasion Task:\")\nf1 = evaluate_f1_scores(res['v2_default'], evasion_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:02:04.674318Z","iopub.execute_input":"2026-02-15T20:02:04.674676Z","iopub.status.idle":"2026-02-15T20:02:04.697461Z","shell.execute_reply.started":"2026-02-15T20:02:04.674633Z","shell.execute_reply":"2026-02-15T20:02:04.696724Z"}},"outputs":[{"name":"stdout","text":"Evaluating evasion Task:\nMacro F1-Score:    0.0367\nWeighted F1-Score: 0.0655\n\n--- Detailed Classification Report ---\n                     precision    recall  f1-score   support\n\n           Explicit       0.00      0.00      0.00        79\n           Implicit       0.00      0.00      0.00        61\n            Dodging       0.20      1.00      0.33        61\n            General       0.00      0.00      0.00        55\n         Deflection       0.00      0.00      0.00        22\nPartial/half-answer       0.00      0.00      0.00         7\nDeclining to answer       0.00      0.00      0.00        11\n   Claims ignorance       0.00      0.00      0.00         8\n      Clarification       0.00      0.00      0.00         4\n\n           accuracy                           0.20       308\n          macro avg       0.02      0.11      0.04       308\n       weighted avg       0.04      0.20      0.07       308\n\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"print(\"Evaluating evasion Task:\")\nf1 = evaluate_f1_scores(res['v2_persona'], evasion_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T20:21:08.329968Z","iopub.execute_input":"2026-02-15T20:21:08.330232Z","iopub.status.idle":"2026-02-15T20:21:08.354549Z","shell.execute_reply.started":"2026-02-15T20:21:08.330195Z","shell.execute_reply":"2026-02-15T20:21:08.353724Z"}},"outputs":[{"name":"stdout","text":"Evaluating evasion Task:\nMacro F1-Score:    0.0367\nWeighted F1-Score: 0.0655\n\n--- Detailed Classification Report ---\n                     precision    recall  f1-score   support\n\n           Explicit       0.00      0.00      0.00        79\n           Implicit       0.00      0.00      0.00        61\n            Dodging       0.20      1.00      0.33        61\n            General       0.00      0.00      0.00        55\n         Deflection       0.00      0.00      0.00        22\nPartial/half-answer       0.00      0.00      0.00         7\nDeclining to answer       0.00      0.00      0.00        11\n   Claims ignorance       0.00      0.00      0.00         8\n      Clarification       0.00      0.00      0.00         4\n\n           accuracy                           0.20       308\n          macro avg       0.02      0.11      0.04       308\n       weighted avg       0.04      0.20      0.07       308\n\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"evasion_labels = ['Explicit', 'Implicit', 'Dodging', 'General', 'Deflection', \n                  'Partial/half-answer', 'Declining to answer', 'Claims ignorance', 'Clarification']\nprint(\"Evaluating evasion Task:\")\nf1 = evaluate_f1_scores(res['v2_definition'], evasion_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T21:40:36.256577Z","iopub.execute_input":"2026-02-15T21:40:36.257660Z","iopub.status.idle":"2026-02-15T21:40:36.281933Z","shell.execute_reply.started":"2026-02-15T21:40:36.257610Z","shell.execute_reply":"2026-02-15T21:40:36.281339Z"}},"outputs":[{"name":"stdout","text":"Evaluating evasion Task:\nMacro F1-Score:    0.0825\nWeighted F1-Score: 0.1669\n\n--- Detailed Classification Report ---\n                     precision    recall  f1-score   support\n\n           Explicit       0.24      0.58      0.34        79\n           Implicit       0.22      0.36      0.27        61\n            Dodging       0.31      0.08      0.13        61\n            General       0.00      0.00      0.00        55\n         Deflection       0.00      0.00      0.00        22\nPartial/half-answer       0.00      0.00      0.00         7\nDeclining to answer       0.00      0.00      0.00        11\n   Claims ignorance       0.00      0.00      0.00         8\n      Clarification       0.00      0.00      0.00         4\n\n           accuracy                           0.24       308\n          macro avg       0.09      0.11      0.08       308\n       weighted avg       0.17      0.24      0.17       308\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"del model, tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}